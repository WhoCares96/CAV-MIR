{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_names = [\n",
    "    \"gender_female\",\n",
    "    \"gender_male\",\n",
    "    \"language_de\",\n",
    "    \"language_en\",\n",
    "    #    \"language_es\",\n",
    "    #    \"language_fr\",\n",
    "    #    \"language_it\",\n",
    "    #    \"language_ja\",\n",
    "    #    \"language_pt\",\n",
    "    #    \"singer_age_decade_10\",\n",
    "    #    \"singer_age_decade_20\",\n",
    "    # \"singer_age_decade_30\",\n",
    "    # \"singer_age_decade_40\",\n",
    "    #    \"singer_age_decade_50\",\n",
    "    #    \"singer_age_decade_60\",\n",
    "    #    \"singer_age_decade_70\",\n",
    "    #    \"singer_age_decade_80\",\n",
    "]\n",
    "encoder_ids = [(\"mert_v1_95m\", 768), (\"whisper_v2_large\", 1280)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create CAVs using LDA for Significance Test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training meta parameters\n",
    "num_train_runs = 500\n",
    "training_sample_count = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import itertools\n",
    "\n",
    "import s3fs\n",
    "from cavmir.utils import (\n",
    "    create_in_memory_test_dataloader,\n",
    "    lda_one_cav,\n",
    "    load_df_and_embeddings,\n",
    "    store_cav_vector_array,\n",
    "    store_evaluation_metrics,\n",
    ")\n",
    "\n",
    "s3 = s3fs.S3FileSystem(anon=False)\n",
    "\n",
    "dataset_prefix = os.environ[\"DATASET_PREFIX\"]\n",
    "embedding_prefix = os.environ[\"EMBEDDING_PREFIX\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fitting project: gender_female\n",
      "Iteration 500/500\n",
      "Fitting project: gender_male\n",
      "Iteration 500/500\n",
      "Fitting project: language_de\n",
      "Iteration 500/500\n",
      "Fitting project: language_en\n",
      "Iteration 500/500"
     ]
    }
   ],
   "source": [
    "train_variations = [*itertools.product(project_names, encoder_ids)]\n",
    "\n",
    "\n",
    "for project_name, (encoder_id, embedding_dim) in train_variations:\n",
    "    print(f\"\\nFitting project: {project_name}\")\n",
    "    df = load_df_and_embeddings(\n",
    "        project_name, \"train\", encoder_id, dataset_prefix, embedding_prefix, s3\n",
    "    )\n",
    "\n",
    "    df_test = load_df_and_embeddings(\n",
    "        project_name, \"test\", encoder_id, dataset_prefix, embedding_prefix, s3\n",
    "    )\n",
    "\n",
    "    test_dataloader = create_in_memory_test_dataloader(df_test)\n",
    "\n",
    "    cav_vectors = []\n",
    "    evaluation_metrics = []\n",
    "\n",
    "    for i in range(num_train_runs):\n",
    "        sys.stdout.write(f\"\\rIteration {i + 1}/{num_train_runs}\")\n",
    "\n",
    "        cav_vector, evaluation_metric = lda_one_cav(\n",
    "            random_state=i,\n",
    "            df=df,\n",
    "            project_name=project_name,\n",
    "            training_sample_count=training_sample_count,\n",
    "            embedding_dim=embedding_dim,\n",
    "            test_dataloader=test_dataloader,\n",
    "            plot_evaluation=False,\n",
    "        )\n",
    "\n",
    "        cav_vectors.append(cav_vector)\n",
    "        evaluation_metrics.append(evaluation_metric)\n",
    "\n",
    "    store_cav_vector_array(\n",
    "        cav_vectors, f\"cav_ttest_{project_name}.npy\", encoder_id, project_name\n",
    "    )\n",
    "    store_evaluation_metrics(\n",
    "        evaluation_metrics,\n",
    "        f\"evaluation_metrics_ttest_{project_name}.json\",\n",
    "        encoder_id,\n",
    "        project_name,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train CAV for Qualitative Tests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_split = 0.9\n",
    "\n",
    "# training parameters\n",
    "epochs = 500\n",
    "batch_size = 128\n",
    "learning_rate = 0.002\n",
    "embedding_dim = 768\n",
    "dropout_rate = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import s3fs\n",
    "from cavmir.training.dataset import (\n",
    "    create_dataloader_from_webdataset_path,\n",
    "    create_webdataset,\n",
    ")\n",
    "from cavmir.utils import (\n",
    "    append_embeddings_to_df,\n",
    "    create_training_samples_from_df,\n",
    "    store_cav_vector_array,\n",
    "    store_evaluation_metrics,\n",
    "    train_one_cav,\n",
    ")\n",
    "\n",
    "s3 = s3fs.S3FileSystem(anon=False)\n",
    "\n",
    "dataset_prefix = os.environ[\"DATASET_PREFIX\"]\n",
    "embedding_prefix = os.environ[\"EMBEDDING_PREFIX\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training set\n",
    "df = pd.read_csv(os.path.join(dataset_prefix, f\"train_dataset_{project_name}.csv\"))\n",
    "df = append_embeddings_to_df(df, embedding_prefix, encoder_id, s3)\n",
    "\n",
    "training_sample_count = int(len(df) * train_val_split)\n",
    "validation_sample_count = len(df) - training_sample_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "537"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_sample_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create test set for evaluation\n",
    "df_test = pd.read_csv(os.path.join(dataset_prefix, f\"test_dataset_{project_name}.csv\"))\n",
    "df_test = append_embeddings_to_df(df_test, embedding_prefix, encoder_id, s3)\n",
    "\n",
    "create_webdataset(\n",
    "    create_training_samples_from_df(df_test),\n",
    "    f\"datasets/{encoder_id}_test_{project_name}.tar\",\n",
    ")\n",
    "\n",
    "test_dataloader = create_dataloader_from_webdataset_path(\n",
    "    f\"datasets/{encoder_id}_test_{project_name}.tar\", batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4833 537\n",
      "Epoch 1/500\n",
      "Epoch 1 Train Loss: 0.6953\n",
      "Epoch 1 Validation Loss: 0.6983\n",
      "Model saved to trainings/singer_age_decade_30/state_dict.pth\n",
      "Epoch 11/500\n",
      "Epoch 11 Train Loss: 0.6680\n",
      "Epoch 11 Validation Loss: 0.6871\n",
      "Model saved to trainings/singer_age_decade_30/state_dict.pth\n",
      "Epoch 21/500\n",
      "Epoch 21 Train Loss: 0.6578\n",
      "Epoch 21 Validation Loss: 0.6804\n",
      "Model saved to trainings/singer_age_decade_30/state_dict.pth\n",
      "Epoch 31/500\n",
      "Epoch 31 Train Loss: 0.6513\n",
      "Epoch 31 Validation Loss: 0.6763\n",
      "Model saved to trainings/singer_age_decade_30/state_dict.pth\n",
      "Epoch 41/500\n",
      "Epoch 41 Train Loss: 0.6465\n",
      "Epoch 41 Validation Loss: 0.6737\n",
      "Model saved to trainings/singer_age_decade_30/state_dict.pth\n",
      "Epoch 51/500\n",
      "Epoch 51 Train Loss: 0.6427\n",
      "Epoch 51 Validation Loss: 0.6720\n",
      "Model saved to trainings/singer_age_decade_30/state_dict.pth\n",
      "Epoch 61/500\n",
      "Epoch 61 Train Loss: 0.6395\n",
      "Epoch 61 Validation Loss: 0.6709\n",
      "Model saved to trainings/singer_age_decade_30/state_dict.pth\n",
      "Epoch 71/500\n",
      "Epoch 71 Train Loss: 0.6367\n",
      "Epoch 71 Validation Loss: 0.6701\n",
      "Model saved to trainings/singer_age_decade_30/state_dict.pth\n",
      "Epoch 81/500\n",
      "Epoch 81 Train Loss: 0.6343\n",
      "Epoch 81 Validation Loss: 0.6695\n",
      "Model saved to trainings/singer_age_decade_30/state_dict.pth\n",
      "Epoch 91/500\n",
      "Epoch 91 Train Loss: 0.6321\n",
      "Epoch 91 Validation Loss: 0.6692\n",
      "Model saved to trainings/singer_age_decade_30/state_dict.pth\n",
      "Epoch 101/500\n",
      "Epoch 101 Train Loss: 0.6302\n",
      "Epoch 101 Validation Loss: 0.6689\n",
      "Model saved to trainings/singer_age_decade_30/state_dict.pth\n",
      "Epoch 111/500\n",
      "Epoch 111 Train Loss: 0.6261\n",
      "Epoch 111 Validation Loss: 0.6654\n",
      "Model saved to trainings/singer_age_decade_30/state_dict.pth\n",
      "Epoch 121/500\n",
      "Epoch 121 Train Loss: 0.6257\n",
      "Epoch 121 Validation Loss: 0.6654\n",
      "Model saved to trainings/singer_age_decade_30/state_dict.pth\n",
      "Epoch 131/500\n",
      "Epoch 131 Train Loss: 0.6257\n",
      "Epoch 131 Validation Loss: 0.6654\n",
      "Early stopping at epoch 134\n",
      "Loss history saved to trainings/singer_age_decade_30/loss_history.json\n"
     ]
    }
   ],
   "source": [
    "cav_vector, evaluation_metric = train_one_cav(\n",
    "    train_index=0,\n",
    "    df=df,\n",
    "    project_name=project_name,\n",
    "    encoder_id=encoder_id,\n",
    "    target_positive_class=target_positive_class,\n",
    "    training_sample_count=training_sample_count,\n",
    "    validation_sample_count=validation_sample_count,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    learning_rate=learning_rate,\n",
    "    embedding_dim=embedding_dim,\n",
    "    dropout_rate=dropout_rate,\n",
    "    test_dataloader=test_dataloader,\n",
    ")\n",
    "\n",
    "\n",
    "store_cav_vector_array(cav_vector, f\"cav_full_{project_name}.npy\", project_name)\n",
    "store_evaluation_metrics(\n",
    "    evaluation_metric, f\"evaluation_metrics_full_{project_name}.json\", project_name\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
