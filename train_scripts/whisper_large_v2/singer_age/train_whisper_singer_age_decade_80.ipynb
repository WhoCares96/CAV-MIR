{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prerequisites\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import dotenv\n",
        "\n",
        "dotenv.load_dotenv()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "project_name = \"singer_age_decade_80\"\n",
        "encoder_id = \"whisper_large_v2\"\n",
        "\n",
        "\n",
        "# data preparation\n",
        "target_column = \"singer_age_decade\"\n",
        "target_positive_class = 80\n",
        "\n",
        "\n",
        "# training meta parameters\n",
        "num_train_runs = 100\n",
        "training_sample_count = 100\n",
        "\n",
        "\n",
        "# training parameters\n",
        "epochs = 500\n",
        "batch_size = training_sample_count\n",
        "learning_rate = 0.002\n",
        "embedding_dim = 1280\n",
        "dropout_rate = 0.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Prepare Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "import pandas as pd\n",
        "import s3fs\n",
        "from cavmir.training.dataset import (\n",
        "    create_dataloader_from_webdataset_path,\n",
        "    create_webdataset,\n",
        ")\n",
        "from cavmir.utils import (\n",
        "    append_embeddings_to_df,\n",
        "    create_training_samples_from_df,\n",
        "    train_one_cav,\n",
        ")\n",
        "\n",
        "s3 = s3fs.S3FileSystem(anon=False)\n",
        "\n",
        "dataset_prefix = os.environ[\"DATASET_PREFIX\"]\n",
        "embedding_prefix = os.environ[\"EMBEDDING_PREFIX\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# load training set\n",
        "df = pd.read_csv(os.path.join(dataset_prefix, f\"train_dataset_{project_name}.csv\"))\n",
        "df = append_embeddings_to_df(df, embedding_prefix, encoder_id, s3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# create test set for evaluation\n",
        "df_test = pd.read_csv(os.path.join(dataset_prefix, f\"test_dataset_{project_name}.csv\"))\n",
        "df_test = append_embeddings_to_df(df_test, embedding_prefix, encoder_id, s3)\n",
        "\n",
        "create_webdataset(\n",
        "    create_training_samples_from_df(df_test),\n",
        "    f\"datasets/{encoder_id}_test_{project_name}.tar\",\n",
        ")\n",
        "\n",
        "test_dataloader = create_dataloader_from_webdataset_path(\n",
        "    f\"datasets/{encoder_id}_test_{project_name}.tar\", batch_size=batch_size\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training run 1/100\n",
            "Epoch 1/500\n",
            "Epoch 1 Train Loss: 0.6957\n",
            "Epoch 1 Validation Loss: 0.6892\n",
            "Model saved to trainings/singer_age_decade_10/state_dict.pth\n",
            "Epoch 11/500\n",
            "Epoch 11 Train Loss: 0.6449\n",
            "Epoch 11 Validation Loss: 0.6742\n",
            "Model saved to trainings/singer_age_decade_10/state_dict.pth\n",
            "Epoch 21/500\n",
            "Epoch 21 Train Loss: 0.5998\n",
            "Epoch 21 Validation Loss: 0.6622\n",
            "Model saved to trainings/singer_age_decade_10/state_dict.pth\n",
            "Epoch 31/500\n",
            "Epoch 31 Train Loss: 0.5601\n",
            "Epoch 31 Validation Loss: 0.6540\n",
            "Model saved to trainings/singer_age_decade_10/state_dict.pth\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 5\u001b[39m\n",
            "\u001b[32m      2\u001b[39m evaluation_metrics = []\n",
            "\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_train_runs):\n",
            "\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     cav_vector, evaluation_metric = \u001b[43mtrain_one_cav\u001b[49m\u001b[43m(\u001b[49m\n",
            "\u001b[32m      6\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n",
            "\u001b[32m      7\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\n",
            "\u001b[32m      8\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproject_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproject_name\u001b[49m\u001b[43m,\u001b[49m\n",
            "\u001b[32m      9\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_id\u001b[49m\u001b[43m,\u001b[49m\n",
            "\u001b[32m     10\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtarget_column\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtarget_column\u001b[49m\u001b[43m,\u001b[49m\n",
            "\u001b[32m     11\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtarget_positive_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtarget_positive_class\u001b[49m\u001b[43m,\u001b[49m\n",
            "\u001b[32m     12\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_train_runs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_train_runs\u001b[49m\u001b[43m,\u001b[49m\n",
            "\u001b[32m     13\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtraining_sample_count\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtraining_sample_count\u001b[49m\u001b[43m,\u001b[49m\n",
            "\u001b[32m     14\u001b[39m \u001b[43m        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n",
            "\u001b[32m     15\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n",
            "\u001b[32m     16\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n",
            "\u001b[32m     17\u001b[39m \u001b[43m        \u001b[49m\u001b[43membedding_dim\u001b[49m\u001b[43m=\u001b[49m\u001b[43membedding_dim\u001b[49m\u001b[43m,\u001b[49m\n",
            "\u001b[32m     18\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdropout_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdropout_rate\u001b[49m\u001b[43m,\u001b[49m\n",
            "\u001b[32m     19\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m,\u001b[49m\n",
            "\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[32m     22\u001b[39m     cav_vectors.append(cav_vector)\n",
            "\u001b[32m     23\u001b[39m     evaluation_metrics.append(evaluation_metric)\n",
            "\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/git/CAV-MIR/packages/cavmir/cavmir/utils/__init__.py:197\u001b[39m, in \u001b[36mtrain_one_cav\u001b[39m\u001b[34m(train_index, df, project_name, encoder_id, target_column, target_positive_class, num_train_runs, training_sample_count, epochs, batch_size, learning_rate, embedding_dim, dropout_rate, test_dataloader)\u001b[39m\n",
            "\u001b[32m    187\u001b[39m val_dataloader = create_dataloader_from_webdataset_path(\n",
            "\u001b[32m    188\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mdatasets/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mencoder_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_val_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mproject_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.tar\u001b[39m\u001b[33m\"\u001b[39m, batch_size=batch_size\n",
            "\u001b[32m    189\u001b[39m )\n",
            "\u001b[32m    191\u001b[39m model = CAVNetwork(\n",
            "\u001b[32m    192\u001b[39m     input_shape=embedding_dim,\n",
            "\u001b[32m    193\u001b[39m     target_shape=\u001b[32m1\u001b[39m,\n",
            "\u001b[32m    194\u001b[39m     dropout_rate=dropout_rate,\n",
            "\u001b[32m    195\u001b[39m )\n",
            "\u001b[32m--> \u001b[39m\u001b[32m197\u001b[39m \u001b[43mfit_cav_model\u001b[49m\u001b[43m(\u001b[49m\n",
            "\u001b[32m    198\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n",
            "\u001b[32m    199\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\n",
            "\u001b[32m    200\u001b[39m \u001b[43m    \u001b[49m\u001b[43mval_dataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n",
            "\u001b[32m    201\u001b[39m \u001b[43m    \u001b[49m\u001b[43mout_files_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrainings/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mproject_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n",
            "\u001b[32m    202\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n",
            "\u001b[32m    203\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n",
            "\u001b[32m    204\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n",
            "\u001b[32m    205\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[32m    207\u001b[39m evaluation_metrics = evaluate_cav_model(\n",
            "\u001b[32m    208\u001b[39m     model=model,\n",
            "\u001b[32m    209\u001b[39m     test_dataloader=test_dataloader,\n",
            "\u001b[32m    210\u001b[39m     true_label_name=target_positive_class,\n",
            "\u001b[32m    211\u001b[39m     loss_history_dir=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mtrainings/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mproject_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/loss_history.json\u001b[39m\u001b[33m\"\u001b[39m,\n",
            "\u001b[32m    212\u001b[39m )\n",
            "\u001b[32m    214\u001b[39m cav_vector = model.get_concept_activation_vector()\n",
            "\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/git/CAV-MIR/packages/cavmir/cavmir/training/fit.py:62\u001b[39m, in \u001b[36mfit_cav_model\u001b[39m\u001b[34m(model, train_dataset, val_dataset, out_files_dir, num_epochs, learning_rate, early_stopping_patience, device, verbose_steps)\u001b[39m\n",
            "\u001b[32m     59\u001b[39m model.train()\n",
            "\u001b[32m     60\u001b[39m epoch_loss = \u001b[32m0.0\u001b[39m\n",
            "\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
            "\u001b[32m     63\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_samples_from_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[32m     65\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/git/CAV-MIR/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:708\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n",
            "\u001b[32m    705\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "\u001b[32m    706\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n",
            "\u001b[32m    707\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n",
            "\u001b[32m--> \u001b[39m\u001b[32m708\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[32m    709\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n",
            "\u001b[32m    710\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n",
            "\u001b[32m    711\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n",
            "\u001b[32m    712\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "\u001b[32m    713\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n",
            "\u001b[32m    714\u001b[39m ):\n",
            "\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/git/CAV-MIR/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1458\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n",
            "\u001b[32m   1455\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._process_data(data)\n",
            "\u001b[32m   1457\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._tasks_outstanding > \u001b[32m0\u001b[39m\n",
            "\u001b[32m-> \u001b[39m\u001b[32m1458\u001b[39m idx, data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[32m   1459\u001b[39m \u001b[38;5;28mself\u001b[39m._tasks_outstanding -= \u001b[32m1\u001b[39m\n",
            "\u001b[32m   1460\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable:\n",
            "\u001b[32m   1461\u001b[39m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
            "\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/git/CAV-MIR/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1420\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._get_data\u001b[39m\u001b[34m(self)\u001b[39m\n",
            "\u001b[32m   1416\u001b[39m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n",
            "\u001b[32m   1417\u001b[39m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n",
            "\u001b[32m   1418\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "\u001b[32m   1419\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
            "\u001b[32m-> \u001b[39m\u001b[32m1420\u001b[39m         success, data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[32m   1421\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m success:\n",
            "\u001b[32m   1422\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
            "\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/git/CAV-MIR/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1251\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._try_get_data\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n",
            "\u001b[32m   1238\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout=_utils.MP_STATUS_CHECK_INTERVAL):\n",
            "\u001b[32m   1239\u001b[39m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n",
            "\u001b[32m   1240\u001b[39m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n",
            "\u001b[32m   (...)\u001b[39m\u001b[32m   1248\u001b[39m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n",
            "\u001b[32m   1249\u001b[39m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n",
            "\u001b[32m   1250\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
            "\u001b[32m-> \u001b[39m\u001b[32m1251\u001b[39m         data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_data_queue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[32m   1252\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n",
            "\u001b[32m   1253\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "\u001b[32m   1254\u001b[39m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n",
            "\u001b[32m   1255\u001b[39m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n",
            "\u001b[32m   1256\u001b[39m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
            "\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/multiprocessing/queues.py:113\u001b[39m, in \u001b[36mQueue.get\u001b[39m\u001b[34m(self, block, timeout)\u001b[39m\n",
            "\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m block:\n",
            "\u001b[32m    112\u001b[39m     timeout = deadline - time.monotonic()\n",
            "\u001b[32m--> \u001b[39m\u001b[32m113\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n",
            "\u001b[32m    114\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n",
            "\u001b[32m    115\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._poll():\n",
            "\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/multiprocessing/connection.py:257\u001b[39m, in \u001b[36m_ConnectionBase.poll\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n",
            "\u001b[32m    255\u001b[39m \u001b[38;5;28mself\u001b[39m._check_closed()\n",
            "\u001b[32m    256\u001b[39m \u001b[38;5;28mself\u001b[39m._check_readable()\n",
            "\u001b[32m--> \u001b[39m\u001b[32m257\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
            "\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/multiprocessing/connection.py:440\u001b[39m, in \u001b[36mConnection._poll\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n",
            "\u001b[32m    439\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_poll\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout):\n",
            "\u001b[32m--> \u001b[39m\u001b[32m440\u001b[39m     r = \u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[32m    441\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(r)\n",
            "\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/multiprocessing/connection.py:1136\u001b[39m, in \u001b[36mwait\u001b[39m\u001b[34m(object_list, timeout)\u001b[39m\n",
            "\u001b[32m   1133\u001b[39m     deadline = time.monotonic() + timeout\n",
            "\u001b[32m   1135\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
            "\u001b[32m-> \u001b[39m\u001b[32m1136\u001b[39m     ready = \u001b[43mselector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[32m   1137\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ready:\n",
            "\u001b[32m   1138\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m [key.fileobj \u001b[38;5;28;01mfor\u001b[39;00m (key, events) \u001b[38;5;129;01min\u001b[39;00m ready]\n",
            "\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/selectors.py:415\u001b[39m, in \u001b[36m_PollLikeSelector.select\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n",
            "\u001b[32m    413\u001b[39m ready = []\n",
            "\u001b[32m    414\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
            "\u001b[32m--> \u001b[39m\u001b[32m415\u001b[39m     fd_event_list = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_selector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[32m    416\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n",
            "\u001b[32m    417\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
            "\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "cav_vectors = []\n",
        "evaluation_metrics = []\n",
        "\n",
        "for i in range(num_train_runs):\n",
        "    cav_vector, evaluation_metric = train_one_cav(\n",
        "        train_index=i + 1,\n",
        "        df=df,\n",
        "        project_name=project_name,\n",
        "        encoder_id=encoder_id,\n",
        "        target_column=target_column,\n",
        "        target_positive_class=target_positive_class,\n",
        "        num_train_runs=num_train_runs,\n",
        "        training_sample_count=training_sample_count,\n",
        "        epochs=epochs,\n",
        "        batch_size=batch_size,\n",
        "        learning_rate=learning_rate,\n",
        "        embedding_dim=embedding_dim,\n",
        "        dropout_rate=dropout_rate,\n",
        "        test_dataloader=test_dataloader,\n",
        "    )\n",
        "\n",
        "    cav_vectors.append(cav_vector)\n",
        "    evaluation_metrics.append(evaluation_metric)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "np.save(\n",
        "    os.path.join(\n",
        "        \"trainings\",\n",
        "        project_name,\n",
        "        f\"cav_{project_name}.npy\",\n",
        "    ),\n",
        "    np.array(cav_vectors),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "json.dump(\n",
        "    evaluation_metrics,\n",
        "    open(\n",
        "        os.path.join(\n",
        "            \"trainings\",\n",
        "            project_name,\n",
        "            f\"evaluation_metrics_{project_name}.json\",\n",
        "        ),\n",
        "        \"w\",\n",
        "    ),\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
